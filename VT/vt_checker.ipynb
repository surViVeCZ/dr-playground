{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to run your async code in the notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "import subprocess\n",
    "from typing import List, Tuple, Optional\n",
    "import pyarrow.parquet as pq\n",
    "import vt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # tqdm.notebook for Jupyter notebook\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfMerger\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'malign'  # You can change this to 'benign' to read from the benign dataset\n",
    "\n",
    "# Maximum of api calls for VirusTotal, current academic api is 20k per day\n",
    "batch_size = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomainAnalyzer\n",
    "**Objective**: Define the `DomainAnalyzer` class that will handle domain analysis tasks.\n",
    "\n",
    "- **Functions Included**:\n",
    "    - `__init__`: Initializes the `DomainAnalyzer` with a VirusTotal API key loaded from an environment variable.\n",
    "    - `__enter__` and `__exit__`: Context management methods to handle the setup and cleanup of the client.\n",
    "    - `initialize_client`: Load API key and initialize the vt.Client.\n",
    "    - `check_domain`: Fetch information for a specific domain.\n",
    "    - `get_verdict`: Determine the verdict of the analysis based on the domain's analysis stats.\n",
    "    - `is_domain_live`: Check if a domain is live by calling a bash script.\n",
    "    - `extract_domain_data`: Extract necessary data from the domain result.\n",
    "    - `load_previous_data`: Load previously processed domain data from a CSV file.\n",
    "    - `save_data`: Save the DataFrame containing domain data to a CSV file.\n",
    "    - `generate_report`: Generate a report based on the DataFrame and save it as a PDF.\n",
    "    - `process_selected_domains`: Process the domains based on the mode ('malign' or 'benign') in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DomainAnalyzer with a VirusTotal API key loaded from an environment variable.\n",
    "        \"\"\"\n",
    "        self.client = self.initialize_client()\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Enter the runtime context for the DomainAnalyzer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"\n",
    "        Exit the runtime context for the DomainAnalyzer and ensure the client is closed.\n",
    "        \"\"\"\n",
    "        self.client.close()  # Close the vt.Client\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_client():\n",
    "        \"\"\"\n",
    "        Load API key and initialize the vt.Client.\n",
    "        \"\"\"\n",
    "        load_dotenv()  # Load environment variables from .env file\n",
    "        api_key = os.getenv('VT_API_KEY')  # Get the API key from the environment variable\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key is not set. Please set the VT_API_KEY environment variable.\")\n",
    "\n",
    "        return vt.Client(api_key)\n",
    "    \n",
    "    def check_domain(self, domain: str) -> Optional[vt.Object]:\n",
    "        \"\"\"\n",
    "        Fetch information for a specific domain.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url_id = vt.url_id(domain)\n",
    "            with self.client as client:  # Using the client as a context manager\n",
    "                obj = client.get_object(f\"/urls/{url_id}\")\n",
    "            return obj\n",
    "        except vt.APIError as e:\n",
    "            print(f\"Error: Unable to fetch information for domain {domain}. {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_verdict(self, analysis_stats: dict) -> str:\n",
    "        \"\"\"\n",
    "        Determine the verdict of the analysis.\n",
    "        \"\"\"\n",
    "        if analysis_stats.get('malicious', 0) > 0 or analysis_stats.get('suspicious', 0) > 1:\n",
    "            return \"Malign\"\n",
    "        else:\n",
    "            return \"Benign\"\n",
    "        \n",
    "    def is_domain_live(self, domain: str) -> str:\n",
    "        \"\"\"\n",
    "        Check if a domain is live by calling a bash script.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Running the bash script and capturing the output\n",
    "            result = subprocess.run(['./livetest.sh', domain], capture_output=True, text=True)\n",
    "            output = result.stdout.strip()\n",
    "            if output == '1':\n",
    "                return \"Alive\"\n",
    "            else:\n",
    "                return \"Dead\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Unable to check if domain {domain} is live. {e}\")\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def extract_domain_data(self, domain: str, result: vt.Object) -> Tuple:\n",
    "        \"\"\"\n",
    "        Extract necessary data from the domain result.\n",
    "        \"\"\"\n",
    "        analysis_stats = result.last_analysis_stats\n",
    "        verdict = self.get_verdict(analysis_stats)\n",
    "        detection_ratio = f\"{analysis_stats['malicious']}/{analysis_stats['malicious'] + analysis_stats['harmless']}\"\n",
    "        detection_timestamp = result.last_analysis_date\n",
    "        domain_status = self.is_domain_live(domain)\n",
    "        return domain, verdict, detection_ratio, detection_timestamp, analysis_stats.get('harmless', 0), \\\n",
    "               analysis_stats.get('malicious', 0), analysis_stats.get('suspicious', 0), domain_status\n",
    "    \n",
    "    def load_previous_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load previously processed domain data from a CSV file.\n",
    "        \"\"\"\n",
    "        previous_data_filename = 'previous_data.csv'\n",
    "        if os.path.exists(previous_data_filename):\n",
    "            return pd.read_csv(previous_data_filename)\n",
    "        else:\n",
    "            columns = [\"Domain\", \"Verdict\", \"Detection Ratio\", \"Detection Timestamp\", \"Harmless\", \"Malicious\", \"Suspicious\", \"Live Status\"]\n",
    "            return pd.DataFrame(columns=columns)\n",
    "\n",
    "    def save_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Save the DataFrame containing domain data to a CSV file.\n",
    "        \"\"\"\n",
    "        df.to_csv('previous_data.csv', index=False)\n",
    "\n",
    "    def generate_report(self, df: pd.DataFrame, output_filename: str, rows_per_page: int = 500) -> None:\n",
    "        \"\"\"\n",
    "        Generate a report based on the DataFrame and save it as a PDF.\n",
    "        \"\"\"\n",
    "        pdf_merger = PdfMerger()\n",
    "\n",
    "        num_pages = math.ceil(len(df) / rows_per_page)\n",
    "        for page in range(num_pages):\n",
    "            start_row = page * rows_per_page\n",
    "            end_row = start_row + rows_per_page\n",
    "            page_df = df[start_row:end_row]\n",
    "\n",
    "        previous_data_filename = 'previous_data.csv'\n",
    "        if os.path.exists(previous_data_filename):\n",
    "            old_df = pd.read_csv(previous_data_filename)\n",
    "            merged_df = pd.concat([old_df, df]).drop_duplicates(subset=['Domain']).reset_index(drop=True)\n",
    "        else:\n",
    "            merged_df = df\n",
    "\n",
    "        # Save the merged data for future use\n",
    "        merged_df.to_csv(previous_data_filename, index=False)\n",
    "\n",
    "        benign_count = len(df[df['Verdict'] == 'Benign'])\n",
    "        malign_count = len(df[df['Verdict'] == 'Malign'])\n",
    "        total_count = len(df)\n",
    "        \n",
    "        benign_row = pd.DataFrame([['', 'Benign count', f'{benign_count}/{total_count}', '', '', '', '', '']], columns=df.columns)\n",
    "        malign_row = pd.DataFrame([['', 'Malign count', f'{malign_count}/{total_count}', '', '', '', '', '']], columns=df.columns)\n",
    "        \n",
    "        df = pd.concat([df, benign_row, malign_row], ignore_index=True)\n",
    "        # Adjust the height of the figure based on the number of rows in the DataFrame\n",
    "        fig_height = len(page_df) * 0.5\n",
    "        fig, ax = plt.subplots(figsize=(12, fig_height))\n",
    "        ax.axis('off')  # Hide axes\n",
    "        \n",
    "        colWidths = [max(df[\"Domain\"].apply(len)*0.30) * 0.02 if column == \"Domain\" \n",
    "             else 0.15 if column == \"Detection Timestamp\" \n",
    "             else 0.10 for column in df.columns]\n",
    "        \n",
    "        tab = pd.plotting.table(ax, df, loc='upper center', colWidths=colWidths, cellLoc='center', rowLoc='center')\n",
    "        tab.auto_set_font_size(True) \n",
    "        tab.set_fontsize(8)  \n",
    "        tab.scale(1.2, 1.2)\n",
    "\n",
    "        # Style adjustments (bold headers, colors based on verdict, hiding index)\n",
    "        for key, cell in tab.get_celld().items():\n",
    "            if key[0] == 0 or key[1] == -1:\n",
    "                cell._text.set_weight('bold')\n",
    "            if cell.get_text().get_text() == 'Malign':\n",
    "                cell._text.set_color('red')\n",
    "            elif cell.get_text().get_text() == 'Benign':\n",
    "                cell._text.set_color('green')\n",
    "            if key[1] == -1:\n",
    "                cell.set_visible(False)\n",
    "            if key[0] in [total_count+1, total_count+2]:  # Special styling for the benign and malign count rows\n",
    "                cell._text.set_weight('bold')\n",
    "                cell.set_facecolor('lightgrey')\n",
    "            if cell.get_text().get_text() == 'Dead':\n",
    "                cell._text.set_color('red')\n",
    "            elif cell.get_text().get_text() == 'Alive':\n",
    "                cell._text.set_color('green')\n",
    "        \n",
    "        # Save the table as a PDF\n",
    "        page_filename = f\"temp_page_{page+1}.pdf\"\n",
    "        plt.savefig(page_filename, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Add each page to the merger\n",
    "        pdf_merger.append(page_filename)\n",
    "\n",
    "        # Save the merged PDF\n",
    "        with open(output_filename, 'wb') as merged_pdf:\n",
    "            pdf_merger.write(merged_pdf)\n",
    "\n",
    "        # Clean up the temporary files, if any\n",
    "        for page in range(num_pages):\n",
    "            page_filename = f\"temp_page_{page+1}.pdf\"\n",
    "            if os.path.exists(page_filename):\n",
    "                os.remove(page_filename)\n",
    "\n",
    "    def process_selected_domains(self, mode: str, batch_size) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the domains based on the mode ('malign' or 'benign') in batches.\n",
    "        \"\"\"\n",
    "        paths = {\n",
    "            'malign': '../floor/misp_2307.parquet',\n",
    "            'benign': '../floor/benign_cesnet_union_2307.parquet'\n",
    "        }\n",
    "\n",
    "        # Check if the mode is valid\n",
    "        if mode not in paths:\n",
    "            print(f\"Invalid mode '{mode}'. Please use 'malign' or 'benign'.\")\n",
    "            return\n",
    "\n",
    "        # Read the selected Parquet file and get the domain names\n",
    "        table = pq.read_table(paths[mode])\n",
    "        domain_names = table.column('domain_name').to_pandas()\n",
    "\n",
    "        # Load the processed domains\n",
    "        processed_domains_file = 'processed_domains.txt'\n",
    "        if os.path.exists(processed_domains_file):\n",
    "            with open(processed_domains_file, 'r') as file:\n",
    "                processed_domains = file.read().splitlines()\n",
    "        else:\n",
    "            processed_domains = []\n",
    "\n",
    "        data = []\n",
    "        processed_in_this_run = 0\n",
    "        for domain in tqdm(domain_names, desc='Processing domains', unit='domain'):\n",
    "            if processed_in_this_run >= batch_size:  # Check if the batch size is reached\n",
    "                break  # Exit the loop if the batch size is reached\n",
    "            if domain not in processed_domains:\n",
    "                result = self.check_domain(domain)\n",
    "                if result:\n",
    "                    data.append(self.extract_domain_data(domain, result))\n",
    "                    processed_domains.append(domain)  # Mark domain as processed\n",
    "                    processed_in_this_run += 1  # Increment the processed counter\n",
    "\n",
    "\n",
    "        # Save the updated processed domains\n",
    "        with open(processed_domains_file, 'w') as file:\n",
    "            file.write('\\n'.join(processed_domains))\n",
    "\n",
    "        columns = [\"Domain\", \"Verdict\", \"Detection Ratio\", \"Detection Timestamp\", \"Harmless\", \"Malicious\", \"Suspicious\", \"Live Status\"]\n",
    "        \n",
    "        # Create a DataFrame from the newly processed data\n",
    "        new_df = pd.DataFrame(data, columns=columns)\n",
    "        old_df = self.load_previous_data()\n",
    "\n",
    "        if old_df.empty:\n",
    "            merged_df = new_df\n",
    "        elif new_df.empty:\n",
    "            merged_df = old_df\n",
    "        else:\n",
    "            merged_df = pd.concat([old_df, new_df]).drop_duplicates(subset=['Domain']).reset_index(drop=True)\n",
    "        \n",
    "        merged_df.sort_values(by=['Verdict', 'Live Status'], ascending=[False, False], inplace=True)\n",
    "        # Save the merged data\n",
    "        self.save_data(merged_df)\n",
    "        #print how many domains were processed in total, also include percentages\n",
    "        print(f\"Total number of domains processed: {len(merged_df)} out of {len(domain_names)} ({len(merged_df)/len(domain_names)*100:.2f}%)\")\n",
    "        return merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Utilize the `DomainAnalyzer` class to process and analyze domains.\n",
    "\n",
    "- **Steps**:\n",
    "    1. Instantiate the `DomainAnalyzer` class.\n",
    "    2. Use the `process_selected_domains` method to process domains based on the specified mode and batch size.\n",
    "    3. If domains are processed successfully, generate and save a report using the `generate_report` method.\n",
    "\n",
    "**Note**: Ensure that you have the necessary files, API keys, and configurations before running this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798edb8df2b641bc8cc924541f04a374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing domains:   0%|          | 0/36993 [00:00<?, ?domain/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to fetch information for domain logwarees--plisplease.repl.co. ('NotFoundError', 'URL \"bG9nd2FyZWVzLS1wbGlzcGxlYXNlLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain ug.interperucoumne.repl.co. ('NotFoundError', 'URL \"dWcuaW50ZXJwZXJ1Y291bW5lLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain illas.repl.co. ('NotFoundError', 'URL \"aWxsYXMucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain logwarees.plisplease.repl.co. ('NotFoundError', 'URL \"bG9nd2FyZWVzLnBsaXNwbGVhc2UucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain owngrades--rojoynegro.repl.co. ('NotFoundError', 'URL \"b3duZ3JhZGVzLS1yb2pveW5lZ3JvLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain owngrades.rojoynegro.repl.co. ('NotFoundError', 'URL \"b3duZ3JhZGVzLnJvam95bmVncm8ucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ddress--xascasdas.repl.co. ('NotFoundError', 'URL \"ZGRyZXNzLS14YXNjYXNkYXMucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ddress.xascasdas.repl.co. ('NotFoundError', 'URL \"ZGRyZXNzLnhhc2Nhc2Rhcy5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain atalog.sfa3454.repl.co. ('NotFoundError', 'URL \"YXRhbG9nLnNmYTM0NTQucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ivedistro--ress82.repl.co. ('NotFoundError', 'URL \"aXZlZGlzdHJvLS1yZXNzODIucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ivedistro.ress82.repl.co. ('NotFoundError', 'URL \"aXZlZGlzdHJvLnJlc3M4Mi5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain egacy.cclsi9.repl.co. ('NotFoundError', 'URL \"ZWdhY3kuY2Nsc2k5LnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain egahertz.gordlord.repl.co. ('NotFoundError', 'URL \"ZWdhaGVydHouZ29yZGxvcmQucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ewweb--00494944.repl.co. ('NotFoundError', 'URL \"ZXd3ZWItLTAwNDk0OTQ0LnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain ewweb.00494944.repl.co. ('NotFoundError', 'URL \"ZXd3ZWIuMDA0OTQ5NDQucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain hcbonline.akrdesign.co. ('NotFoundError', 'URL \"aGNib25saW5lLmFrcmRlc2lnbi5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ore11899234.switzerlandnorth.cloudapp.azure.com. ('NotFoundError', 'URL \"b3JlMTE4OTkyMzQuc3dpdHplcmxhbmRub3J0aC5jbG91ZGFwcC5henVyZS5jb20\" not found')\n",
      "Error: Unable to fetch information for domain ore1189923.switzerlandnorth.cloudapp.azure.com. ('NotFoundError', 'URL \"b3JlMTE4OTkyMy5zd2l0emVybGFuZG5vcnRoLmNsb3VkYXBwLmF6dXJlLmNvbQ\" not found')\n",
      "Error: Unable to fetch information for domain 00f74ba44baef2a33a53f6faa85c0cd3256e3b0a68-apidata.googleusercontent.com. ('NotFoundError', 'URL \"MDBmNzRiYTQ0YmFlZjJhMzNhNTNmNmZhYTg1YzBjZDMyNTZlM2IwYTY4LWFwaWRhdGEuZ29vZ2xldXNlcmNvbnRlbnQuY29t\" not found')\n",
      "Error: Unable to fetch information for domain 00f74ba44b3bbdac8099a208fad8bc80ecfd56cc41-apidata.googleusercontent.com. ('NotFoundError', 'URL \"MDBmNzRiYTQ0YjNiYmRhYzgwOTlhMjA4ZmFkOGJjODBlY2ZkNTZjYzQxLWFwaWRhdGEuZ29vZ2xldXNlcmNvbnRlbnQuY29t\" not found')\n",
      "Error: Unable to fetch information for domain 00f74ba44be8f9b9bb3411da8750379890c6c72b06-apidata.googleusercontent.com. ('NotFoundError', 'URL \"MDBmNzRiYTQ0YmU4ZjliOWJiMzQxMWRhODc1MDM3OTg5MGM2YzcyYjA2LWFwaWRhdGEuZ29vZ2xldXNlcmNvbnRlbnQuY29t\" not found')\n",
      "Error: Unable to fetch information for domain imited-dot-sharepointpdf-367707.uk.r.appspot.com. ('NotFoundError', 'URL \"aW1pdGVkLWRvdC1zaGFyZXBvaW50cGRmLTM2NzcwNy51ay5yLmFwcHNwb3QuY29t\" not found')\n",
      "Error: Unable to fetch information for domain resgateagorabr.eastus.cloudapp.azure.com. ('NotFoundError', 'URL \"cmVzZ2F0ZWFnb3JhYnIuZWFzdHVzLmNsb3VkYXBwLmF6dXJlLmNvbQ\" not found')\n",
      "Error: Unable to fetch information for domain unsystems.hk. ('NotFoundError', 'URL \"dW5zeXN0ZW1zLmhr\" not found')\n",
      "Error: Unable to fetch information for domain ireblocks.finance. ('NotFoundError', 'URL \"aXJlYmxvY2tzLmZpbmFuY2U\" not found')\n",
      "Error: Unable to fetch information for domain oyalmail.tracking-info.co.uk. ('NotFoundError', 'URL \"b3lhbG1haWwudHJhY2tpbmctaW5mby5jby51aw\" not found')\n",
      "Error: Unable to fetch information for domain account.venmo.com.sign-in. ('NotFoundError', 'URL \"YWNjb3VudC52ZW5tby5jb20uc2lnbi1pbg\" not found')\n",
      "Error: Unable to fetch information for domain www.cun.uem.mz. ('NotFoundError', 'URL \"d3d3LmN1bi51ZW0ubXo\" not found')\n",
      "Error: Unable to fetch information for domain 73864.bonactech.com. ('NotFoundError', 'URL \"NzM4NjQuYm9uYWN0ZWNoLmNvbQ\" not found')\n",
      "Error: Unable to fetch information for domain aulavirtualmibanco.freshdesk.com. ('NotFoundError', 'URL \"YXVsYXZpcnR1YWxtaWJhbmNvLmZyZXNoZGVzay5jb20\" not found')\n",
      "Error: Unable to fetch information for domain 20.253.72. ('NotFoundError', 'URL \"MjAuMjUzLjcy\" not found')\n",
      "Error: Unable to fetch information for domain 92.204.144. ('NotFoundError', 'URL \"OTIuMjA0LjE0NA\" not found')\n",
      "Error: Unable to fetch information for domain hido.market. ('NotFoundError', 'URL \"aGlkby5tYXJrZXQ\" not found')\n",
      "Error: Unable to fetch information for domain www.fc2neu.h185503.web222.dogado.net. ('NotFoundError', 'URL \"d3d3LmZjMm5ldS5oMTg1NTAzLndlYjIyMi5kb2dhZG8ubmV0\" not found')\n",
      "Error: Unable to fetch information for domain oarcondicionado.com. ('NotFoundError', 'URL \"b2FyY29uZGljaW9uYWRvLmNvbQ\" not found')\n",
      "Error: Unable to fetch information for domain nimation.on-minting.xyz. ('NotFoundError', 'URL \"bmltYXRpb24ub24tbWludGluZy54eXo\" not found')\n",
      "Error: Unable to fetch information for domain ault.replying-mint.xyz. ('NotFoundError', 'URL \"YXVsdC5yZXBseWluZy1taW50Lnh5eg\" not found')\n",
      "Error: Unable to fetch information for domain 543.sites.google.com. ('NotFoundError', 'URL \"NTQzLnNpdGVzLmdvb2dsZS5jb20\" not found')\n",
      "Error: Unable to fetch information for domain ootium.this-premint.com. ('NotFoundError', 'URL \"b290aXVtLnRoaXMtcHJlbWludC5jb20\" not found')\n",
      "Error: Unable to fetch information for domain 20.171.115. ('NotFoundError', 'URL \"MjAuMTcxLjExNQ\" not found')\n",
      "Error: Unable to fetch information for domain acket.kussmuff6.repl.co. ('NotFoundError', 'URL \"YWNrZXQua3Vzc211ZmY2LnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain rapezoids--cursito.repl.co. ('NotFoundError', 'URL \"cmFwZXpvaWRzLS1jdXJzaXRvLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain rapezoids.cursito.repl.co. ('NotFoundError', 'URL \"cmFwZXpvaWRzLmN1cnNpdG8ucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain igration--nouserman.repl.co. ('NotFoundError', 'URL \"aWdyYXRpb24tLW5vdXNlcm1hbi5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain eguri1ebisa--eb1saplusmaster.repl.co. ('NotFoundError', 'URL \"ZWd1cmkxZWJpc2EtLWViMXNhcGx1c21hc3Rlci5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain igration.nouserman.repl.co. ('NotFoundError', 'URL \"aWdyYXRpb24ubm91c2VybWFuLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain eguri1ebisa.eb1saplusmaster.repl.co. ('NotFoundError', 'URL \"ZWd1cmkxZWJpc2EuZWIxc2FwbHVzbWFzdGVyLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain 3284.repl.co. ('NotFoundError', 'URL \"MzI4NC5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain 26-f4d777af882c6452cf8f08055cfe63e1.jarrodchong.com. ('NotFoundError', 'URL \"MjYtZjRkNzc3YWY4ODJjNjQ1MmNmOGYwODA1NWNmZTYzZTEuamFycm9kY2hvbmcuY29t\" not found')\n",
      "Error: Unable to fetch information for domain malltalk--boliviia9843.repl.co. ('NotFoundError', 'URL \"bWFsbHRhbGstLWJvbGl2aWlhOTg0My5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain malltalk.boliviia9843.repl.co. ('NotFoundError', 'URL \"bWFsbHRhbGsuYm9saXZpaWE5ODQzLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain icroinstruction--bisa032.repl.co. ('NotFoundError', 'URL \"aWNyb2luc3RydWN0aW9uLS1iaXNhMDMyLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain icroinstruction.bisa032.repl.co. ('NotFoundError', 'URL \"aWNyb2luc3RydWN0aW9uLmJpc2EwMzIucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain ebsphereres--registerseguros.repl.co. ('NotFoundError', 'URL \"ZWJzcGhlcmVyZXMtLXJlZ2lzdGVyc2VndXJvcy5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain ebsphereres.registerseguros.repl.co. ('NotFoundError', 'URL \"ZWJzcGhlcmVyZXMucmVnaXN0ZXJzZWd1cm9zLnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain anagement--546815454.repl.co. ('NotFoundError', 'URL \"YW5hZ2VtZW50LS01NDY4MTU0NTQucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain anagement.546815454.repl.co. ('NotFoundError', 'URL \"YW5hZ2VtZW50LjU0NjgxNTQ1NC5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain 21.repl.co. ('NotFoundError', 'URL \"MjEucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain odes--quesecuide88.repl.co. ('NotFoundError', 'URL \"b2Rlcy0tcXVlc2VjdWlkZTg4LnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain odes.quesecuide88.repl.co. ('NotFoundError', 'URL \"b2Rlcy5xdWVzZWN1aWRlODgucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain nalyst--bisa754.repl.co. ('NotFoundError', 'URL \"bmFseXN0LS1iaXNhNzU0LnJlcGwuY28\" not found')\n",
      "Error: Unable to fetch information for domain nalyst.bisa754.repl.co. ('NotFoundError', 'URL \"bmFseXN0LmJpc2E3NTQucmVwbC5jbw\" not found')\n",
      "Error: Unable to fetch information for domain emicolon--894154521.repl.co. ('NotFoundError', 'URL \"ZW1pY29sb24tLTg5NDE1NDUyMS5yZXBsLmNv\" not found')\n",
      "Error: Unable to fetch information for domain emicolon.894154521.repl.co. ('NotFoundError', 'URL \"ZW1pY29sb24uODk0MTU0NTIxLnJlcGwuY28\" not found')\n",
      "Total number of domains processed: 16520 out of 36993 (44.66%)\n",
      "Report saved to ../false_positives/VT/malign_VT_check.pdf\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a Jupyter notebook cell:\n",
    "with DomainAnalyzer() as analyzer:  # Using the analyzer as a context manager\n",
    "    df = analyzer.process_selected_domains(mode, batch_size)  # This should generate your DataFrame df\n",
    "    if df is not None and not df.empty:  # Ensure that df is not empty or None\n",
    "        analyzer.generate_report(df, f'../false_positives/VT/{mode}_VT_check.pdf')  # This will use the DataFrame df\n",
    "        print(f'Report saved to ../false_positives/VT/{mode}_VT_check.pdf')\n",
    "    else:\n",
    "        print(f\"No domains processed for mode '{mode}'. No report generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
